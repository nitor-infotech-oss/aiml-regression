{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "* A decision tree is a tree where each node represents a feature(attribute), each link(branch) represents a decision(rule) and each leaf represents an outcome(categorical(classification) or continues value(regression)).\n",
    "* The whole idea is to create a tree like this for the entire data and process a single outcome at every leaf.\n",
    "* Decision trees are used for both classification and regression problems.\n",
    "* Decision tree algorithm falls under the category of supervised learning algorithms.\n",
    "* There are many algorithms for Decision Tree:\n",
    "  * CART(Classification and Regression Trees): uses Gini Index as metric.\n",
    "  * ID3(Iterative Dichotomiser 3): uses Entropy function and Information gain as metrics.\n",
    "\n",
    "**Get a clear explanation on: https://medium.com/deep-math-machine-learning-ai/chapter-4-decision-trees-algorithms-b93975f7a1f1**\n",
    "\n",
    "\n",
    "## Decision Tree Regression\n",
    "\n",
    "Predict a continuous value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Decision tree regression is not the best model to use on single feature dataset as we will do. It is more adapted to the dataset with more features i.e.high dimensional dataset. But the code that we will be implementing can be used with other datasets with other features as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Position  Level   Salary\n",
      "0   Business Analyst      1    45000\n",
      "1  Junior Consultant      2    50000\n",
      "2  Senior Consultant      3    60000\n",
      "3            Manager      4    80000\n",
      "4    Country Manager      5   110000\n",
      "5     Region Manager      6   150000\n",
      "6            Partner      7   200000\n",
      "7     Senior Partner      8   300000\n",
      "8            C-level      9   500000\n",
      "9                CEO     10  1000000\n",
      "\n",
      "\n",
      "[[ 1]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [10]]\n",
      "\n",
      "\n",
      "[  45000   50000   60000   80000  110000  150000  200000  300000  500000\n",
      " 1000000]\n"
     ]
    }
   ],
   "source": [
    "#no feature scaling needed in decision tree regression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\"Position_Salaries.csv\")\n",
    "print(dataset)\n",
    "print('\\n')\n",
    "x = dataset.iloc[: , 1:-1].values   ##for higher dimension dataset: change splitting values and if you have any missing value \n",
    "y = dataset.iloc[: , -1].values   ##then just apply the \"Taking care of missing data\" of our Preprocessing section,\n",
    "print(x)\n",
    "print('\\n')\n",
    "print(y)\n",
    "\n",
    "\n",
    "\n",
    "##for higher dimension dataset:  if there is any categorical data then you have to apply either the \"Encoding categorical data\" \n",
    "##OneHotEncoded part of our Preprocessing section if there's no order relationship in your categorical variables or apply the,\n",
    "##\"Encoding categorical data\"LabelEncoder of our Preprocessing part if there's an ordered relationship like size of clothes,\n",
    "##or if the position levels collected in strings etc, and you dont have to apply feature scaling as decision tree splits your \n",
    "##features in different ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the decision tree regression model on whole dataset\n",
    "\n",
    "#DecisionTreeRegressor will predict a continuous value\n",
    "#DecisionTreeClassifier will predict a category\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "regressor = DecisionTreeRegressor(random_state = 0)   #fixing the random values with 0\n",
    "regressor.fit(x, y)   #train your DecisionTreeRegressor to understand the correlations between the Level and Salary columns\n",
    "\n",
    "\n",
    "\n",
    "##if you have higher dimension dataset: change 'x' to 'x_train' and 'y' to 'y_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting a new result\n",
    "\n",
    "#we would like to have corresponding salary of the observation 6.5, \n",
    "regressor.predict([[6.5]])   #so to format that single position level 6.5 we would have to put it in a 2d array inside which we will put out observation 6.5\n",
    "\n",
    "\n",
    "\n",
    "##if you have higher dimension dataset: you would have to put other features value besides the salary like 6.5,30,5, where 30 can be age and 5 can be number of viewers\n",
    "##this is how you would predict a single observation when you would have several features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    For this case study the predicted salary is 150000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualising the Decision Tree Regression results(higher resolution)\n",
    "x_grid = np.arange(min(x), max(x), 0.1)\n",
    "x_grid = x_grid.reshape((len(x_grid), 1))\n",
    "plt.scatter(x, y, color='red')   #if feature scaling would have been applied then you would have to change the x and y\n",
    "plt.plot(x_grid, regressor.predict((x_grid)), color='blue')\n",
    "plt.title(\"Truth or Bluff (Decision Tree Regression)\")\n",
    "plt.xlabel(\"Position Level\")\n",
    "plt.ylabel(\"Salary\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "##if you have higher dimension dataset: various features will not be plotted so you would have to delete this visualization part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    All the predicted salaries in a particluar range are same so we have the staircase like curve till last. Here it's not continuous, we jump from the Position Level to the next one on every step of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#curve in low resolution\n",
    "\n",
    "plt.scatter(x, y, color='red')   #if feature scaling would have been applied then you would have to change the x and y\n",
    "plt.plot(x, regressor.predict((x)), color='blue')\n",
    "plt.title(\"Truth or Bluff (Decision Tree Regression)\")\n",
    "plt.xlabel(\"Position Level\")\n",
    "plt.ylabel(\"Salary\")\n",
    "plt.show()\n",
    "\n",
    "#here the predicted salary of the Position Level was just the actual salary itself because it was trained on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
